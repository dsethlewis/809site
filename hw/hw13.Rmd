---
title: 'HW #13: The Publishing Process'
author: "Daniel Lewis"
date: "10/28/2020"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
bibliography: biblio.json
csl: "../apa.csl"
---
## Load Packages

```{r}
# R Markdown
library(rbbt)
```

## 1. Questions and Concerns

Before I start, I must acknowledge that since I'm writing this *after* deciding to leave the program at UNC&mdash;and with it, my career in academia&mdash;I must take a different perspective on this topic. Trepidation and anxiety, or excitement, hardly make sense from my position as an outsider. Instead, I can only view the publishing process with curiosity, relief, and, perhaps, a bittersweet longing. 

It would be too difficult at this point to put myself in an old pair of shoes that no longer fit and pretend I am still the Daniel Lewis of last fall who expected to go through the publishing process. Instead, I will work through this assignment as who I am: a detached, but curious observer.

That said, here are some questions and concerns:

1. Does the publishing process in the organizational sciences serve all its stakeholders equally well? If not, who benefits most and at whose expense?
2. Does the publishing process promote research that is balanced in validity, clarity, relevance, and timeliness?
3. To the extent that the institutions of the publishing process embody a hierarchy of power and prestige, are its offices allocated fairly? Does its structure show respect for the value of equality?
4. Does the publishing process accommodate people with disabilities? More specifically, do people with ADHD, autism, learning disabilities, or physical disabilities (but with strong research abilities) have an equal chance of publishing compared to people without such disabilities?
5. How do I personally feel about leaving before going through the publishing process? Am I relieved at having dodged a bullet or regretful at having lost an opportunity?

@graham_stablein95 suggest another question:

6. What forms of constructive criticism and debate are prevalent within the institutions of the publishing process about the publishing process itself? What, if any, reforms are being considered where the publishing process falls short?

## 2. @schneider95

My reaction to most of these propositions is an internal head nod: "This makes sense and is, overall, a good thing." Two propositions set off alarm bells for me, personally, though, and perhaps they inform my answer to Question 5 above.

### *Proposition 1: Getting published requires persistence*

It was not the general statement about persistence that troubled me but rather the comment about feedback:

> "Negative feedback must be desired and accepted and used as a stimulus to produce a better and more publishable paper. We all claim to want feedback but we mean *positive* feedback. Trying to publish in refereed journals is not a path to positive feedback." [@schneider95, 217]

I have learned the hard way that I have an adverse reaction to interpersonal feedback, both positive and negative. I seem to perceive it all as threatening.^[By interpersonal feedback, I mean feedback from other people. Feedback from a machine, an environment, etc. is purely good because it helps me learn.] Too much interpersonal feedback I have received has been given explicitly or implicitly with the purpose of trying to change me in ways that I could not change. Unfortunately, not only do I perceive it as threatening, but my go-to coping mechanism is avoidance, which tends to make the situation worse.

So a career in which Proposition 1 is tied to interpersonal feedback is probably not right for me. Let me be clear, though, that I think feedback is a good thing, my idiosyncratic tendencies aside.

In addition, I'd like to briefly comment on the primacy of negative feedback, specifically, in the publishing process. Feedback, to my mind, ought to play two roles. First, it should provide instructions (countable noun). That is, it should explain to the researcher how to proceed with the project. Second, it should provide instruction (uncountable noun). In this sense, it should teach a researcher how to be a better researcher, increasing their human capital.

Feedback may play a third, additional, role of giving reviewers a medium through which to respond to the manuscript as a stimulus. A poorly written manuscript may irritate a reviewer, generating an action tendency that a reviewer expresses through negative feedback. In this perspective, it is the reviewer whose needs inform the feedback process, rather than the researcher.

Negative feedback is an excellent tool for the first purpose: it represents a roadmap for further action. Positive feedback, while useful for acknowledging what does *not* need to be changed, does not offer much in the way of direction.

Negative and positive feedback must be used in tandem, however, in pursuit of the second purpose. The desired product of instruction is not only knowledge acquisition but behavior change. We want researchers to learn how to produce better research (knowledge), yes, but we also want them *to actually produce it* (behavior). In the process of behavior change, feedback plays the role of reinforcement and punishment, and nobody learns new behaviors^[Except behaviors that functionally remove the stressor, including avoidance], in this domain or any other, by punishment alone.

I worry some about the third role for feedback, because it has the potential to communicate feedback to a researcher that is actually not helpful at all^[Unless we consider discouraging unskilled researchers a good]. There are certainly factors in place to keep this from happening (e.g., action editors, the reviewer's own inhibitory control), but there are also factors that could promote it (e.g., overtaxed reviewers, power & prestige associated with the office).

Probably some combination of instructions, instruction, and emotional expression are communicated through reviews, with the result that reviews are more negative than is optimal.

### *Proposition 6: Plan to delay gratification, but plan to get it*

The author offers excellent advice for dealing with the motivational challenges in the research cycle: both be prepared to wait and figure out how to enjoy the process. This can work well for those of us with hyperbolically discounted preferences.

But I'm on another level because I have ADHD. My preferences are not just hyperbolically discounted&mdash;they're present-biased with a $\beta$ at least one-and-a-half standard deviations below the mean. Advising me to wait for a reward sounds good but doesn't work in practice. I know many people with ADHD successfully publish, but they must do so by bringing the rewards of research into the present. Making projects interesting and exercising creativity can help; breaking down large projects into intermediate stages is certainly not enough. And additional rewards must be introduced and tied to work (e.g., praise, play, celebration, and tangible incentives like treats).

## 3. The Review Process

### @ketchenjr_02

The author offers an interesting perspective on the issues related to negative feedback I raise above. Whereas @schneider95 claimed that reviews were negative (albeit not overly so), @ketchenjr_02, writing seven years later, argues that they are too positive. It seems that the Academy of Management pushed reviewers to be kind, but in the process, led some reviewers to censor themselves.

I don't think reviewers should simply turn the dial back toward negative feedback. What is needed is simply an awareness of the twin goals of giving instructions and providing instruction. Different manuscripts may demand reviews with different tones. Counter-intuitively, a strong manuscript may need more negative feedback because the steps for revision need to be clear and detailed, while a weak manuscript may need more positive feedback because the author may be an early-career scientist who needs instruction and encouragement even as the manuscript is rejected.

### @feldman04

There is a good reminder about data analysis here:

> "First, present the most obvious and straightforward analyses that are appropriate; then, as necessary, conduct additional analyses to tease out potential reasons for your results." [@feldman04, 4]

There are many reasons to start with the most straightforward analyses, but I'll throw a few out here.

- Fewer parameters mean more degrees of freedom for statistical tests
- Complexity breeds mistakes
- Simple analyses tell powerful stories
- Simplicity correlates with parsimony

Also, as I am now a lay reader of management research, I appreciate being able to understand the statistics. Statistical models that go beyond what I learned in two years of graduate school are likely to be beyond my capacity to comprehend. And I'm sure others find research with simple analyses more accessible, too.

### @seibert06

This article gave me a sense of the highs and lows of the review process. At their best, exchanges with reviewers can be mutually enriching intellectual collaborations, exciting challenges to impress high-prestige individuals, and a much-needed push to excellence. At their worst, they can be sources of disheartening criticism, misleading feedback, and anxiety-inducing interpersonal conflict. And all of this takes place in a high-stakes game of institutional politics, career advancement, and scientific innovation. Sounds pretty exciting when you put it like that.

### @campbell82

A powerful statement in this article was:

> "I think it is a fact that academic departments that evaluate faculty in terms of the number of pieces published hurt the publication system and the research enterprise." [@campbell82, 695]

That certainly sounds like an opinion, but it's one I agree with. There is a reason the field does not advise managers to adopt analogous produce-or-perish systems in their own organizations. First, it's a bad incentive because publishing is influenced by many factors outside an individual's control. Second, it's a bad measure of potential for scientific contribution. It has a low sample size, low variance, and it's difficult to ascribe individual differences as the cause. Also, even as a measure of past performance, number of publications isn't a good measure of scientific contribution.

$$Quantity \neq Impact$$

## 4. Example: Exchanges from Actual Reviews

My first impression is that the reviews are written with a quality themselves worthy of publication in some way. These are not like the casual comments my editor would write on my drafts at my previous job. This a serious piece of work in its own right.

As I read through the back-in-forth in Set 1 (Cybernetic Theory of Stress), I wondered why this level of detail and formalism is necessary when writing to such a small audience. My intuition is that it is due to the lack of familiarity and trust between the people involved. You don't know each other personally; you don't get to know each other very well through the process; you don't work for the same organization; and the structure of the review process, like politics or law, is an adversarial collaboration. Lacking familiarity, you cannot make any assumptions and leave it to chance whether the other party will make the same assumptions. And lacking trust, you cannot leave any doubt about your ability to produce a publication-worthy article or willingness to persevere through a long and grueling process.

An idea struck me while reading your September 12^th^ letter that this is a substantive debate on methodology issues. I think of substantive debate in the scientific community as taking place in formal settings, such as journals, seminars, symposia. People of course talk about their research over drinks and at the water cooler, but if you have something important to say, you try to find an audience. But these review exchanges are essentially private conversations and contain well-researched arguments on methodological issues of general interest. Clearly these exchanges do more than socialize and train the original manuscript author. They are a venue for scientific consensus to emerge.

## 5. Top 10 Keys to Publishing in Scholarly Journals

1. Learn to accept and apply feedback on your research.
1. Structure your work to deliver rewards frequently.
1. Start with basic analyses before developing more sophisticated statistical models.
1. Attach a premium to the quality of your writing.
1. State what you mean explicitly; don't assume a reviewer will guess correctly.
1. Rely on others throughout the process for feedback, copyediting, expertise, emotional support, and motivation.
1. Clarify early and often what your contribution and reasoning are.
1. Move through the process one manageable, interesting step at a time.
1. For the sake of your ego, assume setbacks are the fault of lazy and/or incompetent reviewers.
1. But learn from setbacks anyway.

## References

```{r results = "hide"}
bbt_write_bib("biblio.json", bbt_detect_citations("hw13.Rmd"), overwrite = TRUE)
```

